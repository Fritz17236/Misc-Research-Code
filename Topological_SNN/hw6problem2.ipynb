{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Optimizing Encoders with PyStorm (27 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports for working with matrices and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.ndimage # for processing firing-rate measurements\n",
    "\n",
    "# for building and running SNN simulations\n",
    "#import nengo\n",
    "#import nengo_brainstorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the attached problem set outline, this problem will introduce you to PyStorm, a Python package that directly exposes Braindrop's high-level hardware constructs. In particular, it provides finer control over encoders through _tap points_, which Nengo-Brainstorm doesn't expose. Basic familiarity with PyStorm's network objects is assumed (see first 2 pages of __braindrop_pystorm_documentation.pdf__). \n",
    "\n",
    "First, we will motivate encoder optimization with a simple Nengo simulation, demonstrating the consequences of poorly distributed encoders. Then, you will code functions to create 2D tap-point matrices, estimate the encoding vectors they produce, and test them by decoding $f(\\mathbf{x}) = x_1x_2$. Finally, you will code the algorithm for creating 3+ dimensional tap-point matrices, testing them on the more general class of functions $f(\\mathbf{x}) = \\Pi_i^D x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Importance of Mixing Dimensions (3 pts)\n",
    "Braindrop's encoders are set by defining _anchor encoders_ at tap-points, a subset of synaptic lowpass filters that feed the diffuser. We rely on the diffuser to mix the stimulus' different dimensions when computing nonlinear functions of multidimensional variables. To see this, you will first build a toy model in Nengo to compute $f(\\mathbf{x}) = x_1 x_2$ with 2 ensembles: one that uses the default encoders, which are drawn from a uniform hyperspherical distribution, and another that uses encoders focused around the standard basis vectors $\\pm \\mathbf{e}_1 = [\\pm 1, 0]^\\intercal$ and $\\pm \\mathbf{e}_2 = [0, \\pm 1]^\\intercal$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Standard basis-vector encoders\n",
    "To compare an ensemble with uniformly distributed encoders to an ensemble with standard basis encoders, complete the function `poor_2d_encoders()` below to generate $N$ random unit vectors within an angle $\\pm \\theta$ from $\\pm \\mathbf{e}_1$ or $\\pm \\mathbf{e}_2$.\n",
    " - Call this function with N = 128 and $\\theta=5Â°$ and scatter plot the returned encoders.\n",
    " - Call `ax.set_aspect('equal')` to use a 1:1 aspect-ratio when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poor_2d_encoders(N, theta):\n",
    "    \"\"\"\n",
    "    Returns random 2d unit vectors within an angle theta \n",
    "    from the standard basis vectors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    N: int\n",
    "        Number of encoding vectors to generate\n",
    "    theta: float\n",
    "        Maximum angle (in degrees) between a vector and the standard basis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    encs: numpy matrix\n",
    "        N-by-2 matrix of encoding vectors\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    return encs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Testing encoders with Nengo\n",
    "Now build two 2D ensembles in Nengo, each with $N=128$ neurons, one with the default encoders, and another with the encoders you generated in 2.1.1. To do so, set `encoders=encs` in the `nengo.Ensemble()` constructor. Use the 2D stimulus $\\mathbf{x}(t) = \\frac{1}{\\sqrt{2}}[\\cos(2\\pi t), \\sin(2\\pi t)]$ and run the simulation for $T = 2$. Note that we scale our stimulus by $1/\\sqrt{2}$ to keep $||\\mathbf{x}(t)|| \\leq 1 = ||\\mathbf{e}_i||$ (i.e., so that the projection of the stimulus on the encoders is always less than 1). \n",
    "\n",
    "In addition to driving these two ensembles, create two output nodes with `size_in=1` to receive their decoded spike-rates. Finally, create an additional output node with `size_in=1` to compute the correct signal $f(\\mathbf{x}(t)) = x_1(t)x_2(t)$. Connect the input node directly this node, applying the desired function on its connection. Create probes to measure the output nodes of both ensembles, as well as the output node with the correct signal, filtering each with a time-constant given in `tau_probe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "dims = 2\n",
    "tau_probe = 0.01\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with PyStorm\n",
    "Several parts of the following sections will require you to run code on Braindrop. To do this, follow the procedure for uploading your notebook to the ng-amygdala server. For your convenience, we've included the connection guide with this assignment (__remote_connection.pdf__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** NOTE ***\n",
    "\n",
    "Below we've provided a cell for resetting the chip's connection to the server. In most cases, this code will be unnecessary. However, sometimes when the initial handshake occurs (during the `HAL()` call in the tutorial code below), the USB connection fails and the Python call quickly raises the error messeage `Comm failed to init`. If this happens, run this cell below to reset the connection and try again. This may take a few tries for it to reconnect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo /home/ee207student/usbreset.sh # reset the USB connection, which sometimes dies.\n",
    "# if Comm below is repeatedly & quickly failing to init, you can run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** END NOTE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Importing essential PyStorm libraries\n",
    "\n",
    "# HAL, or Hardware Abstraction Layer, must be imported and initialized\n",
    "from pystorm.hal import HAL\n",
    "HAL = HAL()\n",
    "\n",
    "# data_utils contains a useful function for binning spikes and estimating spike rates\n",
    "from pystorm.hal import data_utils\n",
    "\n",
    "# RunControl objects are responsible for executing command sequences on the chip, such as initiating\n",
    "# tuning curve measurements\n",
    "from pystorm.hal.run_control import RunControl\n",
    "\n",
    "# Graph objects represent properties of your network\n",
    "# so that HAL can allocate resources for them on the chip\n",
    "from pystorm.hal.neuromorph import graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, we've copied some functions from the attached PyStorm walkthrough.\n",
    " - `plot_tap_matrix()` will plot tap-points for each dimension. For a $N$-by-$D$ tap-point matrix, it will need to be called $D$ times separately.\n",
    " - `make_cons_stimulus()` will generate a $D$-dimensional stair-case stimulus that will be helpful in measuring a pool's tuning curves.\n",
    " - `make_sine_stimulus()` generates a $D$-dimensional sinusoidal stimulus, where each dimension is given its own frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tap_matrix(matrix, width, height):\n",
    "    '''\n",
    "    Plots single dimension of a tap-point matrix with a given width and height\n",
    "    '''\n",
    "    plt.imshow(matrix.reshape((height, width)), cmap='bwr', vmin=-1, vmax=1, interpolation='none', aspect='equal')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    ax = plt.gca()\n",
    "    ax.set_yticks(np.arange(0, height, 1))\n",
    "    ax.set_xticks(np.arange(0, width, 1))\n",
    "    ax.set_yticklabels(np.arange(1, height+1, 1));\n",
    "    ax.set_xticklabels(np.arange(1, width+1, 1));\n",
    "    ax.set_yticks(np.arange(-.5, height, 1), minor=True);\n",
    "    ax.set_xticks(np.arange(-.5, width, 1), minor=True);\n",
    "\n",
    "    plt.grid(which='minor', color='k', linestyle='--')\n",
    "\n",
    "def make_cons_stimulus(Din, hold_time, points_per_dim, fmax=1000):\n",
    "    '''\n",
    "    Creates multidimensional staircase-like stimulus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Din: int\n",
    "        Dimensionality\n",
    "    hold_time: float (seconds)\n",
    "        Duration that each sample is applied\n",
    "    points_per_dim: int\n",
    "        Number of points per dimension\n",
    "    fmax: float\n",
    "        Converts |x| < 1 to firing rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inp_times: numpy array\n",
    "        Length-S array of times a new sample starts (in seconds) \n",
    "    inputs: numpy matrix\n",
    "        S-by-Din matrix of samples (in units of firing rate)\n",
    "    '''\n",
    "    total_points = points_per_dim ** Din\n",
    "    stim_rates_1d = np.linspace(-fmax, fmax, points_per_dim).astype(int)\n",
    "    if Din > 1:\n",
    "        stim_rates_mesh = np.meshgrid(*([stim_rates_1d]*Din))\n",
    "    else:\n",
    "        stim_rates_mesh = [stim_rates_1d]\n",
    "    stim_rates = [r.flatten() for r in stim_rates_mesh]\n",
    "    inputs = np.array(stim_rates).T\n",
    "    \n",
    "    inp_times = np.arange(0, total_points) * hold_time\n",
    "    return inp_times, inputs\n",
    "\n",
    "def make_sine_stimulus(Din, base_period, cycles, fmax=1000, input_dt=1e-3):\n",
    "    '''\n",
    "    Creates multidimensional sinusoidal stimulus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Din: int\n",
    "        Dimensionality\n",
    "    base_period: float (seconds)\n",
    "        Period of oscillation for first dimension\n",
    "    cycles: float\n",
    "        Number of cycles the stimulus is generated for\n",
    "    fmax: float\n",
    "        Converts |x| < 1 to firing rate\n",
    "    input_dt: float\n",
    "        Step-size between each consecutive sample (in seconds)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    inp_times: numpy array\n",
    "        Length-S array of times a new sample starts (in seconds) \n",
    "    inputs: numpy matrix\n",
    "        S-by-Din matrix of samples (in units of firing rate)\n",
    "    '''\n",
    "    phi = (1 + np.sqrt(5))/2.0\n",
    "    freq = 1/base_period\n",
    "    f = ((1/phi)**np.arange(Din)) * freq\n",
    "    stim_func = lambda t: np.sin(2*np.pi*f*t)\n",
    "    \n",
    "    sim_time = cycles * base_period\n",
    "\n",
    "    inp_times = np.arange(int(sim_time/input_dt))*input_dt\n",
    "    inputs = np.zeros((len(inp_times), Din))\n",
    "\n",
    "    for index, t in enumerate(inp_times):\n",
    "        v = stim_func(t)\n",
    "\n",
    "        for i, vv in enumerate(v):\n",
    "            ff = int(vv*fmax)\n",
    "            if ff == 0:\n",
    "                ff = 1\n",
    "\n",
    "            inputs[index, i] = float(ff)\n",
    "\n",
    "    return inp_times, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating a 2D Tap Point Matrix (4 pts)\n",
    "In the PyStorm walkthrough (attached), we showed you how to create a 1-D tap-point matrix. You will now extend this to 2-D by creating a encoding vectors distributed uniformly on the unit circle. Just as tap-points' anchor-encoding vectors were drawn from $\\{-1,0,+1\\}$ for the 1-D tap-point matrix, tap-points' anchor-encoders for a 2-D tap-point matrix are drawn from $\\{-1,0,+1\\}^2$. The diffuser interpolates between these anchor encoders, mixing neighbors to create encoding vectors for neurons that lie in-between tap-points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Structured approach\n",
    "Complete the function `tp_mat_2d()` below to create an $N$-by-$2$ tap-point matrix with a tap-point density $1/2^\\rho$ structured as follows: \n",
    "\n",
    "Divide the pool into 4 equal-sized quadrants, with the origin in the center. Assign anchor encoders from the standard basis ($\\mathbf{e}_1 = [1, 0]^\\intercal$ and $\\mathbf{e}_2 = [0, 1]^\\intercal$), such that each quadrant alternates between a pair of 2 orthogonal vectors $\\{(\\mathbf{e}_1, \\mathbf{e}_2), (\\mathbf{e}_1, -\\mathbf{e}_2), (-\\mathbf{e}_1, \\mathbf{e}_2), (-\\mathbf{e}_1, -\\mathbf{e}_2)\\}$. Your algorithm's assignment should look like the following image.\n",
    "\n",
    "<img src=\"2d_tpm.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the array parameters given below and use the `plot_tap_matrix()` function (defined above) to plot both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "height = 16\n",
    "width_height = (width, height)\n",
    "N = width * height\n",
    "Din = 2\n",
    "\n",
    "rho = 0\n",
    "\n",
    "def tp_mat_2d(width, height, rho=0):\n",
    "    '''\n",
    "    Creates a structured 2D tap-point matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    width: int\n",
    "        Width of neuron array\n",
    "    height: int\n",
    "        Height of neuron array\n",
    "    rho: int\n",
    "        1/2**rho is the fraction of synaptic filters to use \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tap_matrix: numpy matrix\n",
    "        width*height x 2 tap-point matrix\n",
    "    '''\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Random approach\n",
    "Now, for the baseline, complete the function `tp_mat_random()` to generate a $D$-dimensional random tap-point matrix. Later, you will compare your structured placement algorithm (from 2.2.1) against this algorithm (with $D = 2$). \n",
    "\n",
    "Keep in mind that Braindrop expects each dimension to have an even number of tap-points, as well as an equal number of $+1$'s and $-1$'s. That may be accomplished by creating a random ranking of the tap points and looping through this ranking, assigning $\\{+\\mathbf{e}_1, -\\mathbf{e}_1, +\\mathbf{e}_2, -\\mathbf{e}_2, ..., +\\mathbf{e}_D, -\\mathbf{e}_D\\}$, and repeating until the last tap-point is assigned an anchor-encoding vecot.\n",
    "\n",
    "After writing the function, call it with the same array parameters (as in 2.1.1) and plot both dimensions of the anchor encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_mat_random(Din, width, height, rho=0):\n",
    "    '''\n",
    "    Creates a random 2D tap-point matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Din: int\n",
    "        Dimensionality\n",
    "    width: int\n",
    "        Width of neuron array\n",
    "    height: int\n",
    "        Height of neuron array\n",
    "    rho: int\n",
    "        1/2**rho is the fraction of synaptic filters to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tap_matrix: numpy matrix\n",
    "        width*height x 2 tap-point matrix\n",
    "    '''\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualizing Encoders (3 pts)\n",
    "Having defined tap-point matrices, we would like to know the resulting encoders. To find out, we use the fact that a neuron's firing-rate increases most rapidly along the direction of its encoding vector. That enables us to estimate our pool's encoders from its neurons' measured tuning-curve by computing the $D$-dimensional gradient. This vector, when normalized, yields the encoder.\n",
    "\n",
    "In this part of the problem, you will carry out this procedure for a 2D pool and produce a scatter plot of estimated encoding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Measuring tuning curves\n",
    "The first step in this process is to collect tuning-curves. Follow a procedure similar to what the PyStorm walkthrough describes (Section 4). Build a PyStorm network, create a tap-point matrix, and initialize a pool, with parameters given below. Then, drive the input node with `make_cons_stimulus()` and create timestamps (in nanoseconds). Calling `run_and_sweep()` will return `(spike_obj, spike_bin_times)`, which can then be parsed with `data_utils.bins_to_rates(spike_obj[pool], spike_bin_times, times_w_end, init_discard_frac=.2)` to get a noisy estimate of the $Q^2$-by-$N$ tuning-curve matrix. To get a robust estimate of a tuning-curve's gradient, use `scipy.ndimage.filters.gaussian_filter()` to smooth the firing rate with width `tc_sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool parameters\n",
    "width = 16\n",
    "height = 16\n",
    "N = width * height      # 256 neurons\n",
    "Din = 2\n",
    "Dout = 1\n",
    "\n",
    "# tap-point density parameter\n",
    "rho = 0\n",
    "\n",
    "# number of samples per dimension\n",
    "Q = 10\n",
    "\n",
    "# training hold time\n",
    "training_hold_time = 0.3\n",
    "\n",
    "# smoothing width\n",
    "tc_sigma = 1\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Estimating encoders\n",
    "You're ready to estimate the pool's encoding vectors from your measured tuning curves by completing the function `estimate_encoders()` below. \n",
    "\n",
    "Reshape each of the $Q^2$-by-$N$ measurement matrix's $N$ columns into a $Q$-by-$Q$ matrix. For a general $D$-dimensional encoder, the matrix should have $D$ indices, each iterating from 1 to $Q$. This reshaped format makes estimating the tuning-curve's $D$-dimensional gradient easier. One way to do that is to take the average of `np.diff()` along each of its $D$ dimensions. That yields noisy scaled versions of the true derivative along each dimension. Take the average along each dimension to filter the noise and normalize to obtain a unit-length encoding vector. \n",
    "\n",
    "To check how reliable your $D$-dimensional gradient estimates are, compute SNR of their components from the finite differences' mean and standard deviation. If any component's SNR is less than some threshold, assign that neuron's encoding-vetor to be `np.nan`. In addition to the estimated encoding vectors, this function should return a list of neurons (indices) with valid encoding vectors.\n",
    "\n",
    "After writing the function, call it on the measured tuning-curves and scatter-plot the result with a 1:1 aspect ratio (as in 2.1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_encoders(A, Q, Din, snr_thresh=0.2):\n",
    "    '''\n",
    "    Estimates encoders for all tuning curves in matrix A\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: numpy matrix\n",
    "        Tuning curve matrix with shape Q**Din -by- N\n",
    "    Q: int\n",
    "        Number of samples per dimensions\n",
    "    Din: int\n",
    "        Number of dimensions\n",
    "    snr_thresh: float\n",
    "        SNR threshold for gradient measurement. Gradients with lower SNR\n",
    "        should set the associated encoder to NaN.\n",
    "    Returns\n",
    "    -------\n",
    "    encoders: numpy matrix\n",
    "        N-by-Din matrix of encoding vectors\n",
    "    valid_enc_idx: Python list\n",
    "        List of neuron indices for neurons with valid encoders (i.e., encoder is not nan)\n",
    "    '''\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Testing with the product function  (4 pts)\n",
    "In 2.1, we demonstrated how essential mixed-dimension encoders are for computing $f(\\mathbf{x}) = x_1x_2$. Now use this function to test your tap-point placement algorithm's efficacy with Nengo-Brainstorm. Do so by calling `nengo_brainstorm.add_params(model)` with your Nengo model to create the dictionary `model.config` that will store all your pool-specific parameters. The following sections detail this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Create the base Nengo model\n",
    "Since Nengo-Brainstorm emulates Nengo's structure and objects, first create a standard Nengo network (similar to 2.1). Use the same input $\\mathbf{x}(t)$ (as in 2.1), a single 2D ensemble `ens`, and a single output node with `size_in=1` for this network. Connect the input to `ens` through synapses with the time-constant `tau_syn` and connect `ens` to the output with function `lambda x: x[0]*x[1]`. Finally, probe the input with time-constant `tau_syn` to model the ensemble's synapses, and probe the output with time-constant `tau_probe` to smooth the decoded spike-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble parameters\n",
    "width = 16\n",
    "heigh = 16\n",
    "N = width * height\n",
    "Din = 2\n",
    "rho = 0\n",
    "\n",
    "# synapse's time-constant\n",
    "tau_syn = 0.02\n",
    "\n",
    "# probe's time-constant\n",
    "tau_probe = 0.05\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Configure parameters and simulate with random anchor-encoders\n",
    "Now that you've set up the base Nengo model, you will now configure its pool's parameters. To do this, call `nengo_brainstorm.add_params(model)` to create the dictionary `model.config`. Use this dictionary to specify additional settings for `model`'s pools (Nengo-Brainstorm instantiates PyStorm objects under-the-hood). First, set the ensemble's width and height via `model.config[ens].width = width` and `model.config[ens].height = height`. Then, use `tp_mat_random()` to create a random 2D tap-point matrix and set `model.config[ens].tap_matrix` equal to it. \n",
    "\n",
    "Finally, set up spike-train filters for computing optimal decoders. Recall from the PyStorm walkthrough (Section 3) that we must filter the stimulus by the pool's synapses, apply the desired function, and then filter the result by the next stage's synapses. As we have just a single layer here, set `model.config[ens].training_input_filter` equal to the ensemble's synaptic time-constant `tau_syn` and `model.config[ens].training_output_filter` equal to the probe's time-constant `tau_probe`.\n",
    "\n",
    "Once these configuration parameters have been set, create your Nengo-Brainstorm Simulator object (with `precompute_inputs=True` to speed up the simulation) and run it for a duration of $T = 4$ seconds. Store the simulation data from the output's probe and `sim.trange()` in their own variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation length\n",
    "T = 4\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Redo the simulation with structured anchor encoders\n",
    "You will now redo this simulation using a structured tap-point matrix. To compare tap-point matrices fairly, use the same pool of neurons on Braindrop. Call `tp_mat_2d()` to generate a structured 2-D tap-point matrix and assign it to `model.config[ens].tap_matrix`. Then re-initialize a Nengo-Brainstorm Simulator, run it for $T = 4$ seconds, and store the simulation data from the output's probe and `sim.trange()` in their own variables.\n",
    "\n",
    "In addition, store the simulation data from the input's probe. You will use it to compute the target function and each decode's RMSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Plot decodes and compare RMSEs\n",
    "You will now solve for decoders and assess their performance. As the (probed) stimulus has already been filtered by the synaptic time-constant `tau_syn`, directly apply the function $f(\\mathbf{x}(t)) = x_1(t)x_2(t)$ to it. Filter the result with `nengo.synapses.Lowpass(tau_probe)` to align it with the filtered spike-trains from Braindrop. This procedure mirrors that in the PyStorm walkthrough (Section 3).\n",
    "\n",
    " - Plot the decodes obtained with random and structured tap-point matrices vs. time $t$. \n",
    "  - Overlay them on the target function (computed with the procedure above). \n",
    "  - Include legends and axes labels.\n",
    " - Print the decodes' RMSE to quantify the structured and random tap-point matrix's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Create a 3+ Dimensional Tap-Point Matrix (7 pts)\n",
    "\n",
    "In 2.2, you wrote a function that creates 2-D tap-point matrices. As $D$ increases, hyperquadrants grow exponentially (as $2^D$). Spanning each one requires interpolating between $D$ standard-basis vectors $\\mathbf{e}_d \\in \\{-1,0,+1\\}^D$, $d = 1,...,D$. Thus, anchor encoding-vectors must be assigned to tap-points on a grid of $D$-sided polygons, such that these polygons share the same anchor encoders where they touch. That makes mapping a $D$-dimensional sphere onto a 2D diffuser more challenging as $D$ increases.\n",
    "\n",
    "The placement algorithm currently used for higher dimensions is heuristic. It simply chooses anchor encoders orthogonal to their neighbors; the number of tap-points per dimension grows with $D$. While conceptually simple, its implementation is moderately involved. To help, we've broken it into these two functions: \n",
    " - `get_anchors()` finds candidates for a given location's anchor-encoding vector.\n",
    " - `nD_tp_matrix()` iterates over valid tap-points and assigns anchor encoders. \n",
    " \n",
    "Below, you'll start with the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Find an orthogonal anchor encoder\n",
    "In order to choose a new anchor encoder orthogonal to its neighbors, we build a list of neighbors with `get_neighbor_encoders()`. It takes a position `(x,y)` and a matrix of anchor encoders already assigned to tap-points `vects`. It returns anchor encoders of `(x,y)`'s already-assigned neighbors. Although position `(x,y)` can have up to 8 neighbors, only 4 have anchor encoders already assigned because assignments are made by raster-scanning through tap-points. Since a $D$-dimensional unit-vector has only $D-1$ mutually-orthogonal unit-vectors, `get_neighbor_encoders()` returns a list of 2 or 3 candidates for anchor encoders when $D = 3$ or $D = 4$, respectively. For $D\\geq 5$, it returns a list of 4 candidates.\n",
    "\n",
    "For $D=3$, `(x,y)`'s two neighbors are to the left and above. For $D=4$, the above-left tap-point is added. For $D\\geq 5$, the addition depends on how the 2-D diffuser is traversed (`assign_dir`). If it's left-to-right and then top-to-bottom (`assign_dir=\"LR\"`), then the above-right tap-point is added. If it's top-to-bottom and then left-to-right (`assign_dir=\"TB\"`), then the below-left tap-point is added. If $D>5$, no tap-point is added because all the already-assigned neighbors have been exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_encoders(x, y, vects, assign_dir):\n",
    "    '''\n",
    "    Gets the anchor encoders of the neighbors to the point (x, y)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: int\n",
    "        Current position along the width axis\n",
    "    y: int\n",
    "        Current position along the height axis\n",
    "    vects: numpy matrix\n",
    "        X-by-Y-by-D matrix of current tap point anchor encoders.\n",
    "        This will fill up completely with nonzero-vectors as the\n",
    "        algorithm proceeds.\n",
    "    assign_dir: string\n",
    "        Either \"LR\" or \"TB\" for the direction in which we are assigning\n",
    "        tap-points (and thus which neighbors to consider). \"LR\" means we scan\n",
    "        left-to-right before moving down the height axis of the tap-point matrix.\n",
    "        \"TB\" means we scan top-to-bottom before moving right across the width axis.\n",
    "        This only changes the neighbor in the 5+ dimensional case.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    elim_vects: Python List\n",
    "        List of length-D numpy arrays corresponding to anchor encoders of \n",
    "        the current position's neighbors\n",
    "    '''\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Select orthogonal vector\n",
    "Now complete `get_orthogonal_vector()` to find an orthogonal anchor encoder. It takes a list of neighboring anchor encoders and returns a standard-basis vector orthogonal to this list, with random sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthogonal_vector(vect_list, D):\n",
    "    '''\n",
    "    Finds a cartesian basis vector which is orthogonal to \n",
    "    the basis vectors in the list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vect_list: Python list of numpy arrays\n",
    "        List of D-dimensional standard basis vectors. List has at most D-1 vectors.\n",
    "    D: int\n",
    "        Dimensionality of encoder space\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    orth_vect: numpy array\n",
    "        Length-D array of one of the 2*D signed basis vectors\n",
    "    '''\n",
    "    # your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Assign anchor-encoding vectors to tap-points\n",
    "With these two helper functions complete, you're ready to write `get_anchors()`. This function takes tap-points-along-width `X`, tap-points-along-height `Y`, and dimensionality `D`. It returns a `X`-by-`Y`-by-`D` matrix `vects` of anchor-encoding vectors. \n",
    "\n",
    "To do so, it traverses the `X`-by-`Y` space, determines the anchor encoders neighboring its current location `(x,y)`, and finds an orthogonal anchor encoder $\\mathbf{e}_{(x,y)}$, choosing its sign randomly. To ensure each dimension's $\\pm 1$ tap-points are balanced, each traversal must place both $\\mathbf{e}_{(i,j)}$ and its oppositely-signed encoder $-\\mathbf{e}_{(x,y)}$. Place the oppositely-signed encoder at $(x',y') = (X-x, Y-y)$. This choice guarantees its orthogonal to its own neighbors by running two parallel but coupled assignment procedures over the full `X`-`Y` space. Thus, set the limits of your nested for-loops to half of the full `X`-`Y` space. \n",
    "\n",
    "Finally, decide the order in which to visit these locations $(x,y)$. A good solution is to scan left-to-right and top-to-bottom when `Y > X`, and to scan top-to-bottom and left-to-right when `X > Y`. Either is appropriate when `X == Y`. The reason for taking this approach is that it minimizes the number of tap-points at the interface between the two parallel assignment procedures, and so minimizes the number of adjacent tap-points with non-orthogonal anchor encoders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(X, Y, D):\n",
    "    '''\n",
    "    Get D-dimensional anchor encoders for all pairs in X x Y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: int\n",
    "        Number of valid tap points along width axis\n",
    "    Y: int\n",
    "        Number of valid tap points along height axis\n",
    "    D: int\n",
    "        Dimensionality of encoding space (D > 3)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vects: numpy matrix\n",
    "        X-by-Y-by-D matrix of anchor encoders for each (x,y) entry\n",
    "    '''\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 Build tap-point matrix\n",
    "You're now finally ready to put all the pieces together to create a $N$-by-$D$ tap-point matrix. Complete `nD_tp_matrix()`. It takes dimensions $D$, pool width $W$ and height $H$, and (optional) density parameter $\\rho$, which sets the fraction of synaptic filters used to $1/2^\\rho$. Naturally, it returns a $N$-by-$D$ tap-point matrix.\n",
    "\n",
    "To do this, call `get_anchors()` to get an ordered matrix of anchor encoders, then loop over tap-points (as in the 1D and 2D case) and assign them encoding vectors.\n",
    "\n",
    "Call this function for $D = 3$ and $D = 4$ and plot all the anchor-encoders' dimensions with `plot_tap_matrix()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pool parameters\n",
    "width = 32\n",
    "height = 32\n",
    "width_height = (width, height)\n",
    "N = width * height      # 256 neurons\n",
    "Din = 2\n",
    "Dout = 1\n",
    "\n",
    "\n",
    "def nD_tp_matrix(D, width, height, rho=0):\n",
    "    '''\n",
    "    Creates tap-point matrix for D-dimensional encoders\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D: int\n",
    "        Dimensionality of encoding space (D > 3)\n",
    "    width: int\n",
    "        Width of neuron array\n",
    "    height: int\n",
    "        Height of neuron array\n",
    "    rho: int\n",
    "        1/2**rho is the fraction of total tap-points the \n",
    "        algorithm should use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tap_matrix: numpy matrix\n",
    "        width*height x D tap-point matrix\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Test with Product Functions (6 pts)\n",
    "As in 2.1 and 2.4, you will use $f(\\mathbf{x}) = \\Pi_i^D x_i$ to test how uniformly the diffuser interpolates the anchor encoders well your algorithm placed across the $D$-dimensional hypersphere.  For both $D=3$ and $D=4$, compare your results to random placement using `tp_mat_random()` (from 2.2.2). The procedures Nengo-Brainstorm  performs when it builds a Simulator object can take quite long for high dimensions. It's faster to build and test your model directly with PyStorm. We've supplied the following functions to simplify this task:\n",
    " - `collect_training_spikes()` time-stamps a $D$-dimensional stimulus and sends it through `input_node`. It then collects spikes from `pool` and returns them along with simulation data.\n",
    " - `compute_decoders()` filters the spikes with `tau_syn_out`, to build a measurement matrix $\\mathbf{A}$; filters the stimulus with `tau_syn_in`, applying the function `func`; and filters the result with `tau_syn_out`, to get the target output $\\mathbf{f}$. It returns the decoders, $\\mathbf{A}$, $\\mathrm{f}$, and a potentially rescaled $f_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_spikes(HAL, net, inp_times, inp_stim, input_node, pool):\n",
    "    FUDGE = 2\n",
    "    curr_time = HAL.get_time()\n",
    "    times = curr_time + FUDGE * 1e9 + inp_times * 1e9\n",
    "    times_w_end = np.hstack((times, times[-1] + input_dt * 1e9))\n",
    "    input_vals = {input_node : (times, inp_stim)}\n",
    "    rc = RunControl(HAL, net)\n",
    "    print(\"getting spikes\")\n",
    "    _, spikes_and_bin_times = rc.run_input_sweep(input_vals, get_raw_spikes=True, \n",
    "                                                 end_time=times_w_end[-1], rel_time=False)\n",
    "    print(\"done\")\n",
    "    spike_dict, spike_bin_times = spikes_and_bin_times\n",
    "    spike_ct = spike_dict[pool]\n",
    "    spikes = spike_ct / input_dt\n",
    "    inp_times_trunc = inp_times[:spikes.shape[0]]\n",
    "    inp_stim_trunc = inp_stim[:spikes.shape[0],:]\n",
    "    \n",
    "    return spikes, inp_times_trunc, inp_stim_trunc\n",
    "\n",
    "def compute_decoders(tau_syn_in, tau_syn_out, spikes, inp_stim, func, fmax):\n",
    "    syn_in = nengo.synapses.Lowpass(tau_syn_in)\n",
    "    syn_out = nengo.synapses.Lowpass(tau_syn_out)\n",
    "\n",
    "    A = syn_out.filt(spikes, y0=0, dt=input_dt)\n",
    "    xin = syn_in.filt(inp_stim, y0=0, dt=input_dt)\n",
    "    fx = np.array([func(xi) for xi in xin])\n",
    "    f = syn_out.filt(fx, y0=0, dt=input_dt)\n",
    "    \n",
    "    solver = nengo.solvers.LstsqL2(reg=0.0001)\n",
    "    decoders, _ = solver(A, f)\n",
    "    max_d = np.max(np.abs(decoders))\n",
    "    if max_d > 1:\n",
    "        fmax_out = fmax / max_d\n",
    "        decoders /= max_d\n",
    "        print(\"rescaling by\", max_d)\n",
    "    else:\n",
    "        fmax_out = fmax\n",
    "    \n",
    "    return decoders, A, f, fmax_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 3-D encoders and $f(\\mathbf{x}) = x_1x_2x_3$\n",
    "Compare your placement algorithm to a random placement for $D = 3$ using a 32-by-32 neuron pool and $\\rho = 1$. Construct two simple encoder-decoder networks `net_struct` and `net_random` (as in section 1.2 of the PyStorm walkthrough) with `Din=3`, a synaptic time-constant `tau_syn=0.02`, and a probe time-constant `tau_probe=0.05`. For `net_struct`, use `nD_tp_matrix()` to create a structured 3-D tap-point matrix. For `net_random`, use `tp_mat_random()` to create a random 3-D tap-point matrix. Connect nodes, pool, and bucket, with initial decode weights of 0. Use `make_sine_stimulus()` to train both networks, with a $2\\pi\\tau_s$ base period, $4(Q/2)^D$ cycles, and $Q = 10$.\n",
    "\n",
    "After calling `HAL.map()`, drive both networks with your stimulus and collect their neurons' spikes using `collect_training_spikes()` (defined above). Compute optimal decoders for $f(\\mathbf{x}) = x_1x_2x_3$ using `compute_decoders()`. Pass it your synapse's and probe's time-constants. Do this for both networks.\n",
    " - Plot both decode's first two seconds and the desired output vs. time $t$. Include a legend and label axes.\n",
    " - Print both decode's RMSE, normalized by the the desired function's peak-to-peak range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to set time resolutions on Braindrop (downstream) and the host (upstream)\n",
    "downstream_time_res = 10000 # ns, or 10us\n",
    "upstream_time_res = 1000000 # ns, or 1ms\n",
    "\n",
    "HAL.set_time_resolution(downstream_time_res, upstream_time_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 4-D encoders and $f(\\mathbf{x}) = x_1x_2x_3x_4$\n",
    "Compare your placement algorithm to a random placement for $D = 4$ using a 32-by-32 neuron pool and $\\rho = 0$.  \n",
    " - Plot both decode's first two seconds and the desired output vs. time $t$. Include a legend and label axes.\n",
    " - Print both decode's RMSE, normalized by the the desired function's peak-to-peak range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
